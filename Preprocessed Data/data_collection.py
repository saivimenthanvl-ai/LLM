# -*- coding: utf-8 -*-
"""data_collection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-5Tpq6_toOXKKnmH8gGjkoncsw9vJXd
"""

from google.colab import drive
drive.mount('/content/drive')

!apt-get update && apt-get install -y poppler-utils

!pip install stanza

import stanza
import numpy as np
import re
from collections import defaultdict
import uuid

stanza.download('ta', verbose=False)
stanza.download('en', verbose=False)
nlp_tamil = stanza.Pipeline('ta', processors='tokenize,pos', verbose=False)
nlp_english = stanza.Pipeline('en', processors='tokenize,pos,ner', verbose=False)

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'
text_output_path = 'extracted_text.txt'
!pdftotext "{pdf_path}" "{text_output_path}"

def apply_nlp(text, language='en'):
    nlp = nlp_english if language == 'en' else nlp_tamil
    doc = nlp(text)
    return [
        [{
            'text': token.text,
            'pos': token.pos,
            # Access 'ner' attribute only if it exists for the token
            'ner': token.ner if hasattr(token, 'ner') else 'O'
        }
        for token in sent.words
        ]
        for sent in doc.sentences
    ]

def is_boilerplate_line(line):
    common_noise = [
    r'உடம்பதாட\.',
    r'https://pandianeducationaltrust\.com/-chenkaantal\.html\.',
    r'திருக்குறள்\.',
    r'ஓராண்டிற்கு',
    r'©\.'
]
    return any(re.search(p, line, re.IGNORECASE) for p in common_noise)

def pos_tag_with_stanza(text):
    doc = nlp(text)
    tagged = []
    for sent in doc.sentences:
        for word in sent.words:
            tagged.append((word.text, word.upos))
    return tagged

def remove_boilerplate(text):
    boilerplate_patterns = [
        r'Bi-Yearly Peer-Reviewed Tamil Jowrmal',
        r'Volume - \d+, Issue - \d+, [A-Za-z]+ \d{4}',
        r'E-ISSN: \d{4}-\d{4}',
        r'DOI: 10\.\d+/zenodo\.\d+',
        r'Received \d+ [A-Za-z]+ \d{4};.*Available online \d+ [A-Za-z]+ \d{4}\.',
        r'Author Contribution Statement:.*?\n',
        r'Author Acknowledgement:.*?\n',
        r'Author Declaration:.*?\n',
        r'\(6\) The content of the article is licensed under.*?\n',
        r'Be Eco-Friendly',
        r'Available at: http://nandianeducationaltrust\.com/chenkaantal\.html',
        r'ORCID: https://orcid\.org/\d{4}-\d{4}-\d{4}-\d{4}',
    ]
    for pattern in boilerplate_patterns:
        text = re.sub(pattern, '', text, flags=re.MULTILINE)
    return text.strip()

def remove_non_tamil_content(text):
    # Keep Tamil (Unicode range: \u0B80-\u0BFF), English (ASCII), and basic punctuation
    pattern = r'[^\u0B80-\u0BFFa-zA-Z0-9\s.,;:"\'()-\[\]]'
    text = re.sub(pattern, '', text)
    return text

def remove_numbers_except_years(text):
    # Replace numbers not part of a 4-digit year (1900-2099)
    text = re.sub(r'\b(?!19\d{2}|20\d{2})\d+\b', '', text)
    return text

def process_nlp(text, lang='ta'):
    nlp = nlp_ta
    doc = nlp(text)
    sentences = []
    for sent in doc.sentences:
        tokens = []
        for token in sent.words:
            tokens.append({
                'text': token.text,
                'pos': token.pos,
                'ner': token.ner if hasattr(token, 'ner') else 'O'
            })
        sentences.append(tokens)
    return sentences

def rule_based_ner(sentences):
    literature_entities = ['Thirukkural', 'Tolkappiyam', 'Tholkappiya Porulathikaram', 'Akananooru']
    for sentence in sentences:
        for token in sentence:
            if token['text'] in literature_entities:
                token['ner'] = 'LITERATURE'
    return sentences

def entity_linking(sentences):
    knowledge_base = {
        'Thirukkural': 'A classic Tamil text by Thiruvalluvar',
        'Tolkappiyam': 'An ancient Tamil grammar and literature text',
        'Tholkappiya Porulathikaram': 'A section of Tolkappiyam on poetics',
        'Akananooru': 'A classical Tamil poetic work'
    }
    for sentence in sentences:
        for token in sentence:
            if token['ner'] == 'LITERATURE' and token['text'] in knowledge_base:
                token['entity_link'] = knowledge_base[token['text']]
    return sentences

def deduplicate_sentences(sentences):
    seen = set()
    deduped = []
    for sentence in sentences:
        sent_text = ' '.join([token['text'] for token in sentence])
        if sent_text not in seen:
            seen.add(sent_text)
            deduped.append(sentence)
    return deduped

def nlp_pipeline(text):
    text = remove_boilerplate(text)
    text = remove_non_tamil_content(text)
    text = remove_numbers_except_years(text)
    sentences = process_nlp(text, lang='ta')
    sentences = rule_based_ner(sentences)
    sentences = entity_linking(sentences)
    sentences = deduplicate_sentences(sentences)
    return sentences

def process_nlp(text, lang='ta'):
    # Use nlp_tamil for Tamil language processing
    nlp = nlp_tamil
    doc = nlp(text)
    sentences = []
    for sent in doc.sentences:
        tokens = []
        for token in sent.words:
            tokens.append({
                'text': token.text,
                'pos': token.pos,
                'ner': token.ner if hasattr(token, 'ner') else 'O'
            })
        sentences.append(tokens)
    return sentences

with open(text_output_path, 'r', encoding='utf-8') as f:
    pdf_text = f.read()

processed_sentences = nlp_pipeline(pdf_text)

for i, sentence in enumerate(processed_sentences):
    print(f"Sentence {i+1}:")
    for token in sentence:
        print(f"Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}")
    print()

with open(text_output_path, 'r', encoding='utf-8') as f:
    pdf_text = f.read()

# Execute Pipeline
processed_data = nlp_pipeline(pdf_path)

# Display Results
for idx, sentence in enumerate(processed_data, 1):
    print(f"Sentence {idx}:")
    for token in sentence:
        print(f"  Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}")
    print()

with open('nlp_output.txt', 'w', encoding='utf-8') as output_file:
    for idx, sentence in enumerate(processed_data, 1):
        output_file.write(f"Sentence {idx}:\n")
        for token in sentence:
            output_file.write(f"  Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}\n")
        output_file.write("\n")

!pip install indic-nlp-library pdfplumber wikipedia-api --quiet

!python -m indicnlp.resources.manager download_resources

import pdfplumber
import re, wikipediaapi
from indicnlp import loader
from indicnlp.tokenize import sentence_tokenize, indic_tokenize

!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

INDIC_NLP_LIB_HOME = '/usr/local/lib/python3.11/dist-packages/indicnlp'
INDIC_NLP_RESOURCES = '/content/indic_nlp_resources'

GAZETTEER_PERSONS = {'அருள்மொழி', 'வந்தியத்தேவன்', 'நந்தினி'}
GAZETTEER_PLACES = {'தஞ்சாவூர்', 'காஞ்சிபுரம்'}

BOILERPLATE_PATTERNS = [
    r'Project Madurai.*?\n', r'Etext.*?\n', r'www.*?\s', r'https?://\S+'
]

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'

def extract_text(pdf):
    text = ''
    with pdfplumber.open(pdf) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += '\n' + page_text
    return text

def remove_boilerplate(text):
    for pattern in BOILERPLATE_PATTERNS:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    return text

def remove_non_tamil(text):
    return ' '.join([w for w in text.split() if re.search(r'[\u0B80-\u0BFF]', w)])

def remove_numbers_but_keep_years(text):
    return ' '.join([w for w in text.split() if not (w.isdigit() and not 1000 <= int(w) <= 2100)])

NOUN_SUFFIXES, VERB_SUFFIXES, PRONOUNS = ['க்கு', 'இல்'], ['கிறான்'], ['நான்', 'நீ']

def pos_tag(token):
    if token in PRONOUNS: return 'PRONOUN'
    if any(token.endswith(s) for s in NOUN_SUFFIXES): return 'NOUN'
    if any(token.endswith(s) for s in VERB_SUFFIXES): return 'VERB'
    return 'OTHER'

def ner_tag(token, next_token=None):
    if token in GAZETTEER_PERSONS: return 'PERSON'
    if token in GAZETTEER_PLACES: return 'LOCATION'
    if next_token in ['நகரம்', 'மாவட்டம்']: return 'POSSIBLE_LOCATION'
    return 'O'

wiki_ta = wikipediaapi.Wikipedia(user_agent='MyTamilNLPApp/1.0 (https://example.com/myappinfo)', language='ta')

def entity_link(entity):
    page = wiki_ta.page(entity)
    return page.fullurl if page.exists() else None

def deduplicate_sentences(sent_list):
    return list(dict.fromkeys(sent_list))

text = extract_text(pdf_path)
text = remove_boilerplate(text)
text = remove_non_tamil(text)
text = remove_numbers_but_keep_years(text)
sentences = sentence_tokenize.sentence_split(text, lang='ta')
sentences = deduplicate_sentences(sentences)

# Deduplication
def deduplicate_sentences(sentences):
    return list(dict.fromkeys(sentences))

#Sentence Tokenization
for sent in sentences:
    tokens = indic_tokenize.trivial_tokenize(sent, lang='ta')
    print(f"\nSentence: {sent.strip()}")
    for idx, tok in enumerate(tokens):
        pos = pos_tag(tok)
        next_tok = tokens[idx+1] if idx+1 < len(tokens) else None
        ner = ner_tag(tok, next_tok)
        link = entity_link(tok) if ner in ['PERSON', 'LOCATION'] else None
        print(f"{tok:15} | POS: {pos:8} | NER: {ner:10} | Link: {link}")

!apt-get install -y tesseract-ocr

!pip install pdfplumber pytesseract indic-nlp-library wikipedia-api --quiet

import pdfplumber
import pytesseract
import re, wikipediaapi
from PIL import Image
from indicnlp import loader
from indicnlp.tokenize import sentence_tokenize, indic_tokenize

INDIC_NLP_LIB_HOME = '/usr/local/lib/python3.11/dist-packages/indicnlp'
INDIC_NLP_RESOURCES = '/usr/local/lib/python3.11/dist-packages/indicnlp/resources'

GAZETTEER_PERSONS = {'அருள்மொழி', 'வந்தியத்தேவன்', 'நந்தினி'}
GAZETTEER_PLACES = {'தஞ்சாவூர்', 'காஞ்சிபுரம்'}

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'

def extract_text_with_ocr(pdf):
    text = ''
    with pdfplumber.open(pdf) as pdf:
        for page in pdf.pages:
            # Try text layer first
            page_text = page.extract_text()
            if page_text and len(page_text.strip()) > 20:
                text += '\n' + page_text
            else:
                print(f"Page {page.page_number} has no text — using OCR.")
                pil_image = page.to_image(resolution=300).original
                ocr_text = pytesseract.image_to_string(pil_image, lang='tam')
                text += '\n' + ocr_text
    return text

def remove_boilerplate(text):
    patterns = [r'Project Madurai.*?\n', r'Etext.*?\n', r'www.*?\s', r'https?://\S+']
    for p in patterns: text = re.sub(p, '', text, flags=re.IGNORECASE)
    return text

def remove_non_tamil(text):
    return ' '.join([w for w in text.split() if re.search(r'[\u0B80-\u0BFF]', w)])

def remove_numbers_but_keep_years(text):
    return ' '.join([w for w in text.split() if not (w.isdigit() and not 1000 <= int(w) <= 2100)])

NOUN_SUFFIXES, VERB_SUFFIXES, PRONOUNS = ['க்கு', 'இல்'], ['கிறான்'], ['நான்', 'நீ']

def pos_tag(token):
    if token in PRONOUNS: return 'PRONOUN'
    if any(token.endswith(s) for s in NOUN_SUFFIXES): return 'NOUN'
    if any(token.endswith(s) for s in VERB_SUFFIXES): return 'VERB'
    return 'OTHER'

def ner_tag(token, next_token=None):
    if token in GAZETTEER_PERSONS: return 'PERSON'
    if token in GAZETTEER_PLACES: return 'LOCATION'
    if next_token in ['நகரம்', 'மாவட்டம்']: return 'POSSIBLE_LOCATION'
    return 'O'

wiki_ta = wikipediaapi.Wikipedia(user_agent='MyTamilNLPApp/1.0 (https://example.com/myappinfo)', language='ta')

def entity_link(entity):
    page = wiki_ta.page(entity)
    return page.fullurl if page.exists() else None

def deduplicate_sentences(sents):
  return list(dict.fromkeys(sents))

text = extract_text_with_ocr(pdf_path)
text = remove_boilerplate(text)
text = remove_non_tamil(text)
text = remove_numbers_but_keep_years(text)
sentences = sentence_tokenize.sentence_split(text, lang='ta')
sentences = deduplicate_sentences(sentences)

for sent in sentences:
    tokens = indic_tokenize.trivial_tokenize(sent, lang='ta')
    print(f"\nSentence: {sent.strip()}")
    for idx, tok in enumerate(tokens):
        pos = pos_tag(tok)
        next_tok = tokens[idx+1] if idx+1 < len(tokens) else None
        ner = ner_tag(tok, next_tok)
        link = entity_link(tok) if ner in ['PERSON', 'LOCATION'] else None
        print(f"{tok:15} | POS: {pos:8} | NER: {ner:10} | Link: {link}")

!pip install langdetect --quiet

!pip install indic-nlp-library --quiet

from indicnlp.tokenize.sentence_tokenize import sentence_split
from indicnlp.tokenize.indic_tokenize import trivial_tokenize
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
#from indicnlp.morph.analyzer import Analyzer
from indicnlp import common
import unicodedata
import re
from langdetect import detect
import json

!pip install stanza

import stanza
stanza.download('ta')
nlp = stanza.Pipeline('ta', processors='tokenize,pos,lemma', use_gpu=False)

def lemmatize_tamil(text):
    doc = nlp(text)
    lemmas = []
    for sentence in doc.sentences:
        for word in sentence.words:
            lemmas.append(word.lemma)
    return lemmas

text = "அவன் சிறந்த பாடல்கள் எழுதுகிறான்"
print(lemmatize_tamil(text))

INDICNLP_RESOURCES = "/path/to/indicnlp/resources"
common.set_resources_path(INDICNLP_RESOURCES)
#analyzer = Analyzer("ta", INDICNLP_RESOURCES + "/morph/ta/")

# Tamil range regex
tamil_range = re.compile(r'[\u0B80-\u0BFF]+')

# Tamil stopwords (example subset)
stopwords_ta = set(['ஒரு', 'என்று', 'இந்த', 'மற்றும்', 'இது'])

def clean_and_process(text):
    text = unicodedata.normalize('NFKC', text)
    if not tamil_range.search(text):
      return None
    try:
      if detect(text) != 'ta':
        return None
    except:
      return None
    sentences = sentence_split(text, lang='ta')
    cleaned_sentences = []

    for sent in sentences:
        # Word tokenize
        words = trivial_tokenize(sent)
        words = [w for w in words if w not in stopwords_ta]

        lemmatized = []
        for word in words:
            # Without Analyzer, just keep the word
            lemmatized.append(word)

        if lemmatized:
            cleaned_sentences.append(" ".join(lemmatized))

    return cleaned_sentences

input_text = "அவன் ஒரு சிறந்த வீரன். அவன் போரில் வெற்றி பெற்றான்."

output = clean_and_process(input_text)

print(output)

