# -*- coding: utf-8 -*-
"""Data Preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-5Tpq6_toOXKKnmH8gGjkoncsw9vJXd
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install requests beautifulsoup4 lxml

import os, time, json, re, unicodedata, hashlib
from urllib.parse import urljoin, urlparse
import requests

from bs4 import BeautifulSoup

SEED_URLS = [
    "https://www.projectmadurai.org/",
    "https://ta.wikisource.org/wiki/%E0%AE%AE%E0%AF%81%E0%AE%95%E0%AE%AA%E0%AF%8D%E0%AE%AA%E0%AF%81",
    "https://www.tamilvu.org/",
]

OUT_DIR = "data_raw"
HEADERS = {"User-Agent": "TamilCorpusCollector/0.1 (academic use; contact: your@email)"}
ALLOWED_HOSTS = {urlparse(u).netloc for u in SEED_URLS}
VISITED = set()

def is_allowed(url):
    host = urlparse(url).netloc
    return any(host.endswith(ah) for ah in ALLOWED_HOSTS)

def looks_like_doc(url):
    return any(url.lower().endswith(ext) for ext in [".html", ".htm", ".php", "/"]) and not any(
        s in url.lower() for s in ["login", "signup", "account", "cart", "pdfviewer"]
    )

def clean_text(html):
    soup = BeautifulSoup(html, "lxml")
    for bad in soup(["script", "style", "noscript", "header", "footer", "nav", "form"]):
        bad.decompose()
    txt = soup.get_text("\n", strip=True)
    return txt

def save_doc(source_url: str, title: str, text: str):
    os.makedirs(OUT_DIR, exist_ok=True)
    h = hashlib.md5(source_url.encode("utf-8")).hexdigest()[:12]
    fn = os.path.join(OUT_DIR, f"{h}.jsonl")
    meta = {
        "title": title or "",
        "author": "",
        "source": source_url,
        "text": unicodedata.normalize("NFC", text),
    }
    with open(fn, "w", encoding="utf-8") as f:
        f.write(json.dumps(meta, ensure_ascii=False) + "\n")
    print("saved:", fn)

def crawl(url: str, depth=0, max_depth=2, delay=1.0):
    if url in VISITED or depth > max_depth or not is_allowed(url):
        return
    VISITED.add(url)
    try:
        r = requests.get(url, headers=HEADERS, timeout=20)
        r.raise_for_status()
    except Exception as e:
        print("skip:", url, e)
        return

    if "text/html" not in r.headers.get("content-type", ""):
        return

    soup = BeautifulSoup(r.text, "lxml")
    title = (soup.title.get_text(strip=True) if soup.title else "").strip()
    if looks_like_doc(url):
        text = clean_text(r.text)
        if len(text) > 200:  # store only contentful pages
            save_doc(url, title, text)

    # enqueue links
    for a in soup.find_all("a", href=True):
        nxt = urljoin(url, a["href"])
        if nxt.startswith("mailto:") or nxt.startswith("javascript:"):
            continue
        crawl(nxt, depth + 1, max_depth, delay)
        time.sleep(delay)

if __name__ == "__main__":
    for seed in SEED_URLS:
        crawl(seed, max_depth=2)

!pip install scrapy readability-lxml

import hashlib, unicodedata
from urllib.parse import urlparse
import scrapy
from readability import Document

SEED_URLS = [
    "https://www.projectmadurai.org/",
    "https://ta.wikisource.org/",
    "https://www.tamilvu.org/",
]
ALLOWED = [urlparse(u).netloc for u in SEED_URLS]

class TamilSpider(scrapy.Spider):
    name = "tamil"
    allowed_domains = ALLOWED
    start_urls = SEED_URLS
    custom_settings = {
        "ROBOTSTXT_OBEY": True,
        "DOWNLOAD_DELAY": 0.5,
        "CONCURRENT_REQUESTS": 8,
        "FEEDS": {
            "data_raw_scrapy/%(name)s-%(time)s.jsonl": {"format": "jsonlines", "encoding": "utf8"},
        },
        "USER_AGENT": "TamilCorpusCollector/0.1 (academic; contact@example.com)"
    }

    def parse(self, response):
        ctype = response.headers.get("Content-Type", b"").decode().lower()
        if "text/html" in ctype:
            try:
                doc = Document(response.text)
                title = doc.short_title()
                main_html = doc.summary()
            except Exception:
                title, main_html = (response.css("title::text").get() or "").strip(), response.text

            text = scrapy.Selector(text=main_html).xpath("string()").get().strip()
            text = unicodedata.normalize("NFC", text)

            if len(text) > 200:
                yield {
                    "title": title,
                    "author": "",
                    "source": response.url,
                    "text": text,
                }

            # follow same-site links
            for href in response.css("a::attr(href)").getall():
                url = response.urljoin(href)
                host = urlparse(url).netloc
                if any(host.endswith(ad) for ad in self.allowed_domains):
                    yield scrapy.Request(url, callback=self.parse)

!pip install readability-lxml

# preprocess_t2.py
import os, re, json, glob, unicodedata, hashlib
from collections import Counter

IN_GLOB = "data_raw/**/*.jsonl"
OUT_SENT_TXT = "clean/cleaned_tamil_corpus.txt"
OUT_JSONL = "clean/cleaned_tamil_corpus.jsonl"
STATS_PATH = "clean/stats.json"
os.makedirs("clean", exist_ok=True)

TAMIL_BLOCK = (0x0B80, 0x0BFF)

def keep_tamil_punct(s: str) -> str:
    out = []
    for ch in s:
        cp = ord(ch)
        if (TAMIL_BLOCK[0] <= cp <= TAMIL_BLOCK[1]) or ch.isspace() or ch in ".!?;:,()\"'–—-[]{}…":
            out.append(ch)
    s = "".join(out)
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n\s*\n+", "\n\n", s)
    return s.strip()

def rm_headers_footers(s: str) -> str:
    patterns = [
        r"Project Madurai.*?$", r"Etext.*?$", r"https?://\S+", r"www\.\S+",
        r"^Page\s*\d+$", r"^\s*\d+\s*$", r"Vol[-– ]\s*\w+"
    ]
    for p in patterns:
        s = re.sub(p, "", s, flags=re.MULTILINE|re.IGNORECASE)
    return re.sub(r"\n\s*\n+", "\n\n", s).strip()

def sentence_split_ta(text: str):
    # basic splitter on Tamil/Latin sentence punctuation
    parts = re.split(r"[\.!\?]|[|।]", text)
    return [p.strip() for p in parts if p.strip()]

def is_mostly_tamil(s: str, thresh=0.5):
    if not s: return False
    total = sum(ch.isalpha() for ch in s)
    ta = sum(0x0B80 <= ord(ch) <= 0x0BFF for ch in s)
    return total == 0 or (ta / max(1,total)) >= thresh

def dedup_sentences(sents):
    seen = set(); out = []
    for s in sents:
        key = re.sub(r"\s+", " ", s.lower()).strip()
        if key and key not in seen:
            seen.add(key); out.append(s)
    return out

def process_one(rec):
    text = unicodedata.normalize("NFC", rec["text"])
    text = rm_headers_footers(text)
    text = keep_tamil_punct(text)
    sents = sentence_split_ta(text)
    sents = [s for s in sents if is_mostly_tamil(s, 0.4)]
    sents = dedup_sentences(sents)
    return sents

def main():
    all_sents = []
    files = glob.glob(IN_GLOB, recursive=True)
    for fp in files:
        with open(fp, "r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                rec = json.loads(line)
                sents = process_one(rec)
                for s in sents:
                    all_sents.append({
                        "title": rec.get("title",""),
                        "author": rec.get("author",""),
                        "source": rec.get("source",""),
                        "text": s
                    })

    # global dedup by sentence text
    by_text = {}
    for r in all_sents:
        key = re.sub(r"\s+", " ", r["text"].lower())
        by_text.setdefault(key, r)

    out_list = list(by_text.values())

    with open(OUT_JSONL, "w", encoding="utf-8") as f:
        for r in out_list:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")

    with open(OUT_SENT_TXT, "w", encoding="utf-8") as f:
        for r in out_list:
            f.write(r["text"] + "\n")

    stats = {
        "files_read": len(files),
        "sentences_total": len(all_sents),
        "sentences_unique": len(out_list),
    }
    with open(STATS_PATH, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("Saved:", OUT_JSONL, OUT_SENT_TXT)
    print("Stats:", stats)

if __name__ == "__main__":
    main()

!apt-get update && apt-get install -y poppler-utils

!pip install stanza

import stanza
import numpy as np
import re
from collections import defaultdict
import uuid

stanza.download('ta', verbose=False)
stanza.download('en', verbose=False)
nlp_tamil = stanza.Pipeline('ta', processors='tokenize,pos', verbose=False)
nlp_english = stanza.Pipeline('en', processors='tokenize,pos,ner', verbose=False)

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'
text_output_path = 'extracted_text.txt'
!pdftotext "{pdf_path}" "{text_output_path}"

def apply_nlp(text, language='en'):
    nlp = nlp_english if language == 'en' else nlp_tamil
    doc = nlp(text)
    return [
        [
            {
                'text': token.text,
                'pos': token.pos,
                # Access 'ner' only if it exists
                'ner': token.ner if hasattr(token, 'ner') else 'O'
            }
            for token in sent.words
        ]
        for sent in doc.sentences
    ]

sample_text = "Murugan is worshipped in Tamil Nadu."
result = apply_nlp(sample_text, language='en')

# Show output
for sent in result:
    for token in sent:
        print(token)

import re

def is_boilerplate_line(line: str):
    common_noise = [
        r'உடம்பதாட\.',
        r'https://pandianeducationaltrust\.com/-chenkaantal\.html\.',
        r'திருக்குறள்\.',
        r'ஓராண்டிற்கு',
        r'©\.'
    ]
    return any(re.search(p, line, re.IGNORECASE) for p in common_noise)
lines = [
    "இது ஒரு சோதனை வரி.",
    "திருக்குறள்.",
    "https://pandianeducationaltrust.com/-chenkaantal.html."
]

for l in lines:
    print(l, "->", is_boilerplate_line(l))

import re

def is_boilerplate_line(line: str) -> bool:
    """
    Check if a line matches common boilerplate patterns.
    Returns True if the line is noise/boilerplate, otherwise False.
    """
    common_noise = [
        r'உடம்பதாட\.',
        r'https://pandianeducationaltrust\.com/-chenkaantal\.html\.',
        r'திருக்குறள்\.',
        r'ஓராண்டிற்கு',
        r'©\.'
    ]
    return any(re.search(p, line, re.IGNORECASE) for p in common_noise)


def clean_text_lines(lines):
    return [line for line in lines if not is_boilerplate_line(line.strip())]


def clean_text_block(text: str) -> str:
    lines = text.splitlines()
    cleaned_lines = clean_text_lines(lines)
    return "\n".join(cleaned_lines)
raw_text = """
இது ஒரு சோதனை வரி.
திருக்குறள்.
இது மற்றொரு வரி.
https://pandianeducationaltrust.com/-chenkaantal.html.
©. Copyright line
இது இறுதி வரி.
"""

cleaned = clean_text_block(raw_text)
print(cleaned)

import stanza
stanza.download('en')
nlp = stanza.Pipeline('en', processors='tokenize,pos')

def pos_tag_with_stanza(text):
    doc = nlp(text)
    tagged = []
    for sent in doc.sentences:
        for word in sent.words:
            tagged.append((word.text, word.upos))
    return tagged

sample_text = "Murugan is worshipped in Tamil Nadu."
tags = pos_tag_with_stanza(sample_text)

for token, pos in tags:
    print(f"{token:12} -> {pos}")

import re

BOILERPLATE_PATTERNS = [
    r'Bi-Yearly Peer-Reviewed Tamil Journal',
    r'Volume\s*-\s*\d+,\s*Issue\s*-\s*\d+,\s*[A-Za-z]+\s+\d{4}',
    r'E-ISSN:\s*\d{4}-\d{4}',
    r'DOI:\s*10\.\d+/zenodo\.\d+',
    r'Received\s+\d+\s+[A-Za-z]+\s+\d{4};.*?Available online\s+\d+\s+[A-Za-z]+\s+\d{4}\.',
    r'Author Contribution Statement:.*?(?=\n\n|\Z)',
    r'Author Acknowledgement:.*?(?=\n\n|\Z)',
    r'Author Declaration:.*?(?=\n\n|\Z)',
    r'\(6\)\s*The content of the article is licensed under.*?(?=\n\n|\Z)',
    r'Be Eco-Friendly',
    r'Available at:\s*https?://\S+',
    r'ORCID:\s*https://orcid\.org/\d{4}-\d{4}-\d{4}-\d{4}',
]
_COMPILED = [re.compile(p, re.IGNORECASE | re.MULTILINE | re.DOTALL) for p in BOILERPLATE_PATTERNS]

def remove_boilerplate(text):
    if not isinstance(text, str):
        return ""
    for rx in _COMPILED:
        text = rx.sub("", text)
    # collapse multiple blank lines
    text = re.sub(r'\n\s*\n+', '\n\n', text)
    return text.strip()

sample = """
Bi-Yearly Peer-Reviewed Tamil Journal
Volume - 12, Issue - 2, March 2023
E-ISSN: 1234-5678
DOI: 10.1234/zenodo.56789

Author Contribution Statement: The author did everything.

Main article content starts here.
"""
cleaned = remove_boilerplate(sample)
print("Preview:\n", cleaned)

in_path  = "/content/drive/MyDrive/tamil_data.txt"
out_path = "/content/drive/MyDrive/tamil_data_cleaned.txt"

with open(in_path, "r", encoding="utf-8", errors="ignore") as f:
    raw = f.read()

cleaned = remove_boilerplate(raw)

with open(out_path, "w", encoding="utf-8") as f:
    f.write(cleaned)

print(f"Saved cleaned text to: {out_path}")
print("Preview:\n", cleaned[:800])

import re

def remove_non_tamil_content(text):
    # Allow Tamil (\u0B80-\u0BFF), English letters, digits, whitespace, and punctuation
    pattern = r'[^\u0B80-\u0BFFa-zA-Z0-9\s.,;:"\'()\-\[\]\n!?%/]'
    cleaned = re.sub(pattern, '', text)

    # Collapse multiple spaces
    cleaned = re.sub(r'[ \t]+', ' ', cleaned)

    return cleaned.strip()
print(remove_non_tamil_content(raw_text))

import re

def remove_numbers_except_years(text: str) -> str:
    """
    Removes all numbers except 4-digit years in the range 1900-2099.
    Preserves years, deletes other numbers.
    """
    # Remove any number that is not a 4-digit year (1900–2099)
    text = re.sub(r'\b(?!19\d{2}\b|20\d{2}\b)\d+\b', '', text)

    # Collapse multiple spaces
    text = re.sub(r'\s+', ' ', text)

    return text.strip()
sample = """
1995 இல், ஏதோ நடந்தது.

தரவுத்தொகுப்பில் 123 மாதிரிகள் உள்ளன.

அடுத்த மதிப்பாய்வு 2024 இல்.

எண் 56 நீக்கப்பட வேண்டும்.
"""

print(remove_numbers_except_years(sample))

import stanza
stanza.download('ta')
nlp_ta = stanza.Pipeline('ta', processors='tokenize,pos')

def process_nlp(text, lang='ta'):
    nlp = nlp_ta
    doc = nlp(text)
    sentences = []
    for sent in doc.sentences:
        tokens = []
        for w in sent.words:
            tokens.append({
                'text': w.text,
                'pos': getattr(w, 'upos', getattr(w, 'xpos', None)),
                'ner': 'O'   # default since no Tamil NER available
            })
        sentences.append(tokens)
    return sentences

text = "முருகன் தமிழ் நாட்டில் வழிபடப்படுகிறார். Murugan is worshipped in Tamil Nadu."
result = process_nlp(text)

for i, sent in enumerate(result, 1):
    print(f"\nSentence {i}")
    for tok in sent:
        print(f"{tok['text']:15}  POS={tok['pos']:<6}  NER={tok['ner']}")

import re

def rule_based_ner(sentences):
    # Known literature entities + some Tamil-script variants
    literature_entities = {
        "LITERATURE": [
            r"\bThirukkural\b", r"திருக்குறள்",
            r"\bTolkappiyam\b", r"\bTholkappiya Porulathikaram\b",
            r"\bAkananooru\b", r"அகநானூறு"
        ]
    }

    for sentence in sentences:
        for token in sentence:
            for label, patterns in literature_entities.items():
                for pat in patterns:
                    if re.search(pat, token['text'], flags=re.IGNORECASE):
                        token['ner'] = label
    return sentences
sentences = [
    [
        {"text": "Thirukkural", "pos": "NOUN", "ner": "O"},
        {"text": "is", "pos": "AUX", "ner": "O"},
        {"text": "famous", "pos": "ADJ", "ner": "O"}
    ],
    [
        {"text": "அகநானூறு", "pos": "NOUN", "ner": "O"},
        {"text": "poem", "pos": "NOUN", "ner": "O"}
    ]
]

result = rule_based_ner(sentences)
for sent in result:
    print(sent)

def entity_linking(sentences):
    # Knowledge base: canonical name -> description
    knowledge_base = {
        'Thirukkural': 'A classic Tamil text by Thiruvalluvar',
        'Tolkappiyam': 'An ancient Tamil grammar and literature text',
        'Tholkappiya Porulathikaram': 'A section of Tolkappiyam on poetics',
        'Akananooru': 'A classical Tamil poetic work'
    }

    # Aliases mapping to canonical names
    aliases = {
        'திருக்குறள்': 'Thirukkural',
        'tolkappiyam': 'Tolkappiyam',
        'tholkappiya porulathikaram': 'Tholkappiya Porulathikaram',
        'akananooru': 'Akananooru',
        'அகநானூறு': 'Akananooru'
    }

    for sentence in sentences:
        for token in sentence:
            if token.get('ner') == 'LITERATURE':
                text_norm = token['text'].strip()
                # Try direct match
                if text_norm in knowledge_base:
                    token['entity_link'] = knowledge_base[text_norm]
                # Try alias match (case-insensitive)
                elif text_norm.lower() in aliases:
                    canonical = aliases[text_norm.lower()]
                    token['entity_link'] = knowledge_base[canonical]
                else:
                    token['entity_link'] = "Unknown literature reference"
    return sentences
sentences = [
    [
        {"text": "திருக்குறள்", "pos": "NOUN", "ner": "LITERATURE"},
        {"text": "is", "pos": "AUX", "ner": "O"}
    ],
    [
        {"text": "Akananooru", "pos": "NOUN", "ner": "LITERATURE"}
    ]
]

linked = entity_linking(sentences)
for sent in linked:
    for tok in sent:
        print(tok)

def deduplicate_sentences(sentences, case_insensitive=True, return_report=False):
    seen = set()
    deduped = []
    removed = 0

    for sentence in sentences:
        sent_text = ' '.join([tok['text'] for tok in sentence]).strip()
        if case_insensitive:
            sent_text = sent_text.lower()
        sent_text = ' '.join(sent_text.split())  # normalize whitespace

        if sent_text not in seen:
            seen.add(sent_text)
            deduped.append(sentence)
        else:
            removed += 1

    if return_report:
        return deduped, {"kept": len(deduped), "removed": removed, "total": len(sentences)}
    return deduped
sentences = [
    [{"text": "Murugan", "pos": "PROPN", "ner": "O"}],
    [{"text": "Murugan", "pos": "PROPN", "ner": "O"}],
    [{"text": "murugan", "pos": "PROPN", "ner": "O"}],
    [{"text": "Tamil", "pos": "PROPN", "ner": "O"}]
]

deduped, stats = deduplicate_sentences(sentences, return_report=True)
print("Stats:", stats)
for s in deduped:
    print([t['text'] for t in s])

def nlp_pipeline(text):
    text = remove_boilerplate(text)
    text = remove_non_tamil_content(text)
    text = remove_numbers_except_years(text)
    sentences = process_nlp(text, lang='ta')
    sentences = rule_based_ner(sentences)
    sentences = entity_linking(sentences)
    sentences = deduplicate_sentences(sentences)
    return sentences
sample = "முருகன் தமிழ் நாட்டில் வழிபடப்படுகிறார். Thirukkural is a classic text."
result = nlp_pipeline(sample)
print(result)

import stanza

# Initialize Tamil pipeline (POS only, since NER is not available)
stanza.download('ta')
nlp_tamil = stanza.Pipeline('ta', processors='tokenize,pos')

def process_nlp(text, lang='ta'):
    doc = nlp_tamil(text)
    sentences = []
    for sent in doc.sentences:
        tokens = []
        for w in sent.words:
            tokens.append({
                'text': w.text,
                'pos': w.upos,
                'ner': 'O'
            })
        sentences.append(tokens)
    return sentences
text = "முருகன் தமிழ் நாட்டில் வழிபடப்படுகிறார்."
result = process_nlp(text)

for sent in result:
    for tok in sent:
        print(f"{tok['text']:15} POS={tok['pos']} NER={tok['ner']}")

with open(text_output_path, 'r', encoding='utf-8') as f:
    pdf_text = f.read()

import stanza
import re

# 1. Init Tamil Stanza pipeline (POS only)
stanza.download('ta')
nlp_tamil = stanza.Pipeline('ta', processors='tokenize,pos')

# 2. Define your helpers (short versions here)
def remove_boilerplate(text):
    return re.sub(r'Bi-Yearly Peer-Reviewed Tamil Journal', '', text)

def remove_non_tamil_content(text):
    return re.sub(r'[^\u0B80-\u0BFFa-zA-Z0-9\s.,;:"\'()\-\[\]\n!?%/]', '', text)

def remove_numbers_except_years(text):
    return re.sub(r'\b(?!19\d{2}\b|20\d{2}\b)\d+\b', '', text)

def process_nlp(text, lang='ta'):
    doc = nlp_tamil(text)
    sentences = []
    for sent in doc.sentences:
        tokens = []
        for w in sent.words:
            tokens.append({
                'text': w.text,
                'pos': w.upos,
                'ner': 'O'   # default since Tamil NER is missing
            })
        sentences.append(tokens)
    return sentences

def rule_based_ner(sentences):
    for sentence in sentences:
        for token in sentence:
            if token['text'] in ['Thirukkural','Tolkappiyam','Akananooru']:
                token['ner'] = 'LITERATURE'
    return sentences

def entity_linking(sentences):
    kb = {'Thirukkural':'Classic text by Thiruvalluvar'}
    for sentence in sentences:
        for token in sentence:
            if token['ner']=='LITERATURE' and token['text'] in kb:
                token['entity_link'] = kb[token['text']]
    return sentences

def deduplicate_sentences(sentences):
    seen, deduped = set(), []
    for s in sentences:
        st = ' '.join([t['text'] for t in s])
        if st not in seen:
            seen.add(st)
            deduped.append(s)
    return deduped

# 3. The orchestrator
def nlp_pipeline(text):
    text = remove_boilerplate(text)
    text = remove_non_tamil_content(text)
    text = remove_numbers_except_years(text)
    sentences = process_nlp(text, lang='ta')
    sentences = rule_based_ner(sentences)
    sentences = entity_linking(sentences)
    sentences = deduplicate_sentences(sentences)
    return sentences

pdf_text = "முருகன் தமிழ் நாட்டில் வழிபடப்படுகிறார். Thirukkural is a classic text."
processed_sentences = nlp_pipeline(pdf_text)
for i, sent in enumerate(processed_sentences, 1):
    print(f"\nSentence {i}")
    for tok in sent:
        print(tok)

for i, sentence in enumerate(processed_sentences):
    print(f"Sentence {i+1}:")
    for token in sentence:
        print(f"Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}")
    print()

with open(text_output_path, 'r', encoding='utf-8') as f:
    pdf_text = f.read()

# Display Results
for idx, sentence in enumerate(processed_sentences, 1):
    print(f"Sentence {idx}:")
    for token in sentence:
        print(f"  Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}")
    print()
processed_data = nlp_pipeline(pdf_text)

with open('nlp_output.txt', 'w', encoding='utf-8') as output_file:
    for idx, sentence in enumerate(processed_data, 1):
        output_file.write(f"Sentence {idx}:\n")
        for token in sentence:
            output_file.write(f"  Token: {token['text']}, POS: {token['pos']}, NER: {token['ner']}, Entity Link: {token.get('entity_link', 'None')}\n")
        output_file.write("\n")

!pip install indic-nlp-library pdfplumber wikipedia-api --quiet

!python -m indicnlp.resources.manager download_resources

import pdfplumber
import re, wikipediaapi
from indicnlp import loader
from indicnlp.tokenize import sentence_tokenize, indic_tokenize

!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git

INDIC_NLP_LIB_HOME = '/usr/local/lib/python3.11/dist-packages/indicnlp'
INDIC_NLP_RESOURCES = '/content/indic_nlp_resources'

PRONOUNS = {"நான்","நீ","அவர்","அவர்கள்","இது","அது"}
NOUN_SUFFIXES = ["ஐ","க்கு","இல்","இன்","உடைய","ஆல்","இருந்து","கள்"]
VERB_SUFFIXES = ["த்தான்","த்தாள்","த்","கிறார்","கிறார்கள்","கிறேன்","வது"]

GAZETTEER_PERSONS = {'அருள்மொழி', 'வந்தியத்தேவன்', 'நந்தினி'}
GAZETTEER_PLACES = {'தஞ்சாவூர்', 'காஞ்சிபுரம்'}
GAZETTEER_PERSONS = {"திரு","திருமதி","மதி"}
GAZETTEER_PLACES  = {"சென்னை","மதுரை","காஞ்சிபுரம்","கோவை","காஞ்சி"}

import re

BOILERPLATE_PATTERNS = [
    r"Project Madurai.*?$",
    r"Etext.*?$",
    r"www\.\S+",
    r"https?://\S+",
    r"Vol[-– ]\s*\w+",
    r"^\s*\d+\s*$",
    r"Page\s*\d+",
]

def remove_boilerplate(text):
    for pattern in BOILERPLATE_PATTERNS:
        text = re.sub(pattern, "", text, flags=re.MULTILINE | re.IGNORECASE)
    # collapse multiple blank lines
    text = re.sub(r"\n\s*\n+", "\n\n", text)
    return text.strip()
sample = """
Project Madurai Digital Library
Etext prepared by XYZ
www.example.com
Vol- II
12

திருக்குறள்
Page 34
"""

print(remove_boilerplate(sample))

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'

import pdfplumber

def extract_text(pdf_path, as_list=False):
    texts = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                # strip trailing spaces, normalize newlines
                cleaned = page_text.strip()
                texts.append(cleaned)

    if as_list:
        return texts
    return "\n".join(texts)
pdf_text = extract_text("/content/drive/MyDrive/Journals/Amma Vazhi, my friend, is not ours Part-3.pdf")
print(pdf_text[:1000])   # show first 1000 characters

pages = extract_text("/content/drive/MyDrive/Journals/Amma Vazhi, my friend, is not ours Part-3.pdf", as_list=True)
print("Number of pages extracted:", len(pages))
print("First page preview:\n", pages[0][:500])

import re

BOILERPLATE_PATTERNS = [
    r"Project Madurai.*?$",
    r"Etext.*?$",
    r"www\.\S+",
    r"https?://\S+",
    r"Vol[-– ]\s*\w+",
    r"^\s*\d+\s*$"
]

def remove_boilerplate(text):
    for pattern in BOILERPLATE_PATTERNS:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)
    # collapse multiple blank lines
    text = re.sub(r'\n\s*\n+', '\n\n', text)
    return text.strip()
input_path = "/content/drive/MyDrive/tamil_data.txt"
output_path = "/content/drive/MyDrive/tamil_data_cleaned.txt"

with open(input_path, "r", encoding="utf-8") as f:
    raw_text = f.read()

cleaned_text = remove_boilerplate(raw_text)

with open(output_path, "w", encoding="utf-8") as f:
    f.write(cleaned_text)

print(f"Cleaned file saved at: {output_path}")
print("Preview:\n", cleaned_text[:500])  # first 500 chars

import re

def remove_non_tamil(text):
    words = text.split()
    tamil_words = [w for w in words if re.search(r'[\u0B80-\u0BFF]', w)]
    return ' '.join(tamil_words)
txt = "திருக்குறள் Thirukkural was written ~2000 years ago in Tamil (2023 edition)."
print("Tamil only:", remove_non_tamil(txt))

def remove_numbers_but_keep_years(text):
    return ' '.join([w for w in text.split() if not (w.isdigit() and not 1000 <= int(w) <= 2100)])

NOUN_SUFFIXES, VERB_SUFFIXES, PRONOUNS = ['க்கு', 'இல்'], ['கிறான்'], ['நான்', 'நீ']

import re
def pos_tag(token: str) -> str:
    token = token.strip()
    token = re.sub(r'[^\u0B80-\u0BFF]', '', token)
    if token in PRONOUNS:
        return 'PRONOUN'
    if any(token.endswith(suf) for suf in VERB_SUFFIXES):
        return 'VERB'
    if any(token.endswith(suf) for suf in NOUN_SUFFIXES):
        return 'NOUN'
    return 'OTHER'
words = ["நான்", "நூலகத்தில்", "படிக்கிறான்", "தமிழ்"]
for w in words:
    print(w, "->", pos_tag(w))

import re

# Example gazetteers (expand as needed)
GAZETTEER_PERSONS = {"திருவள்ளுவர்", "கம்பர்", "இளங்கோ"}
GAZETTEER_PLACES  = {"சென்னை", "மதுரை", "திருச்சி", "காஞ்சிபுரம்"}

def normalize_token(token: str) -> str:
    """Remove punctuation and normalize token for matching."""
    return re.sub(r'[^\u0B80-\u0BFFA-Za-z]', '', token)

def ner_tag(token: str, next_token: str = None) -> str:
    token_norm = normalize_token(token)
    next_norm  = normalize_token(next_token) if next_token else None

    if token_norm in GAZETTEER_PERSONS:
        return "PERSON"
    if token_norm in GAZETTEER_PLACES:
        return "LOCATION"
    if next_norm in {"நகரம்", "மாவட்டம்"}:
        return "POSSIBLE_LOCATION"
    return "O"
tokens = ["திருவள்ளுவர்", "சென்னை", "நகரம்", "தமிழ்"]
for i, tok in enumerate(tokens):
    nxt = tokens[i+1] if i+1 < len(tokens) else None
    print(tok, "->", ner_tag(tok, nxt))

wiki_ta = wikipediaapi.Wikipedia(user_agent='MyTamilNLPApp/1.0 (https://example.com/myappinfo)', language='ta')

import wikipediaapi

wiki_ta = wikipediaapi.Wikipedia(
    user_agent="TamilNLPBot/1.0 (https://github.com/yourname; contact@example.com)",
    language="ta"
)

page = wiki_ta.page("திருக்குறள்")
print("Page Title:", page.title)
print("Summary:", page.summary[:500])  # first 500 chars

def entity_link(entity):
    entity = entity.strip()
    page = wiki_ta.page(entity)

    if page.exists():
        return {
            "title": page.title,
            "url": page.fullurl,
            "summary": page.summary[:200] + "..." if page.summary else ""
        }
    return None
print(entity_link("திருவள்ளுவர்"))
print(entity_link("மதுரை"))

def remove_non_tamil(text):
    return ''.join([ch for ch in text if re.search(r'[\u0B80-\u0BFF]', ch) or ch.isspace() or ch in ".?!,;:"])

raw_text = extract_text(pdf_path)
print("Raw length:", len(raw_text))

text = remove_boilerplate(raw_text)
print("After boilerplate:", len(text))

text = remove_non_tamil(text)
print("After non-Tamil:", len(text))

sentences = [s.strip() for s in text.split("।") if s.strip()]

def deduplicate_sentences(sent_list):
    return list(dict.fromkeys(sent_list))

text = extract_text(pdf_path)
text = remove_boilerplate(text)
text = remove_non_tamil(text)
text = remove_numbers_but_keep_years(text)
sentences = sentence_tokenize.sentence_split(text, lang='ta')
sentences = deduplicate_sentences(sentences)
print(text)
print(sentences)

# Deduplication
def deduplicate_sentences(sentences):
    return list(dict.fromkeys(sentences))

#Sentence Tokenization
for sent in sentences:
    tokens = indic_tokenize.trivial_tokenize(sent, lang='ta')
    print(f"\nSentence: {sent.strip()}")
    for idx, tok in enumerate(tokens):
        pos = pos_tag(tok)
        next_tok = tokens[idx+1] if idx+1 < len(tokens) else None
        ner = ner_tag(tok, next_tok)
        link = entity_link(tok) if ner in ['PERSON', 'LOCATION'] else None
        print(f"{tok:15} | POS: {pos:8} | NER: {ner:10} | Link: {link}")
        print(sentences)

!apt-get install -y tesseract-ocr

!pip install pdfplumber pytesseract indic-nlp-library wikipedia-api --quiet

import pdfplumber
import pytesseract
import re, wikipediaapi
from PIL import Image
from indicnlp import loader
from indicnlp.tokenize import sentence_tokenize, indic_tokenize

INDIC_NLP_LIB_HOME = '/usr/local/lib/python3.11/dist-packages/indicnlp'
INDIC_NLP_RESOURCES = '/usr/local/lib/python3.11/dist-packages/indicnlp/resources'

GAZETTEER_PERSONS = {'அருள்மொழி', 'வந்தியத்தேவன்', 'நந்தினி'}
GAZETTEER_PLACES = {'தஞ்சாவூர்', 'காஞ்சிபுரம்'}

pdf_path = '/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf'

def extract_text_with_ocr(pdf_path='/content/drive/MyDrive/Journals/Ponniyan Selvan Part1.pdf',ocr_lang= "tam+eng",dpi= 300,min_text_len= 20,verbose = True):
    collected = []

    with pdfplumber.open(pdf_path) as pdf_obj:
        for page in pdf_obj.pages:
            # 1) Try text layer (tweak tolerances to capture more text)
            page_text = page.extract_text(x_tolerance=1.5, y_tolerance=1.5)
            if page_text and len(page_text.strip()) >= min_text_len:
                collected.append(page_text.strip())
                continue

            # 2) OCR fallback
            if verbose:
                print(f"Page {page.page_number}: no reliable text layer — using OCR")
            # Render to image for OCR
            page_image = page.to_image(resolution=dpi).original.convert("RGB")
            # Try to deskew/autorotate
            page_image = _deskew_if_needed(page_image, verbose=verbose)

            # Tesseract config: OEM 3 (LSTM), PSM 4 (block of text) is good for pages
            config = "--oem 3 --psm 4"
            try:
                ocr_text = pytesseract.image_to_string(page_image, lang=ocr_lang, config=config)
            except pytesseract.TesseractError as e:
                if verbose:
                    print(f"  ! Tesseract error on page {page.page_number}: {e}")
                ocr_text = ""

            collected.append(ocr_text.strip())

    return _normalize_text("\n\n".join([t for t in collected if t]))
result = nlp_pipeline(extract_text_with_ocr(pdf_path))

def remove_boilerplate(text: str) -> str:
    lines = [ln for ln in text.splitlines() if ln.strip() and not is_boilerplate_line(ln)]
    text = "\n".join(lines)
    return re.sub(r'\n\s*\n+', '\n\n', text).strip()
print(remove_boilerplate)

def boilerplate_line(line):
    s = line.strip()
    if not s:
        return True

    for p in BOILERPLATE_PATTERNS:
        if re.search(p, s, flags=re.IGNORECASE):
            return True
    return False
BOILERPLATE_PATTERNS = [
    r"Project Madurai.*$",
    r"Etext.*$",
    r"^\s*\d+\s*$",
]

print(boilerplate_line)

import re

def remove_non_tamil(text):
    words = text.split()
    tamil_words = [w for w in words if re.search(r'[\u0B80-\u0BFF]', w)]
    return ' '.join(tamil_words)
print(remove_non_tamil)

def remove_numbers_but_keep_years(text):
    def keep_token(token):
        # Strip punctuation around token
        core = re.sub(r'[^\d]', '', token)
        if core.isdigit():
            year = int(core)
            return 1000 <= year <= 2100
        return True

    tokens = text.split()
    kept = [w for w in tokens if keep_token(w)]
    return ' '.join(kept)
print(remove_numbers_but_keep_years)

NOUN_SUFFIXES, VERB_SUFFIXES, PRONOUNS = ['க்கு', 'இல்'], ['கிறான்'], ['நான்', 'நீ']

def remove_non_tamil_content(text):
    out = []
    for ch in text:
        cp = ord(ch)
        if (TAMIL_BLOCK[0] <= cp <= TAMIL_BLOCK[1]) \
           or ch.isspace() \
           or ch in ".,?!;:–—-()[]{}\"'…":
            out.append(ch)
    cleaned = "".join(out)
    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r"\n\s*\n+", "\n\n", cleaned)
    return cleaned.strip()
print(remove_non_tamil_content)

def pos_tag(token):
    token = re.sub(r'[^\u0B80-\u0BFF]', '', token.strip())
    if not token:
        return 'OTHER'

    if token in PRONOUNS:
        return 'PRONOUN'
    if any(token.endswith(suf) for suf in VERB_SUFFIXES):
        return 'VERB'
    if any(token.endswith(suf) for suf in NOUN_SUFFIXES):
        return 'NOUN'
    return 'OTHER'
words = ["நான்", "நூலகத்தில்", "படிக்கிறான்", "தமிழ்", "அவர்கள்"]
for w in words:
    print(w, "->", pos_tag(w))

def ner_tag(token: str, next_token: str = None) -> str:
    t = normalize_token(token)
    n = normalize_token(next_token) if next_token else None

    if t in GAZETTEER_PERSONS:
        return "PERSON"
    if t in GAZETTEER_PLACES:
        return "LOCATION"
    if n in {"நகரம்", "மாவட்டம்"}:
        return "POSSIBLE_LOCATION"
    return "O"
tokens = ["திருவள்ளுவர்", "சென்னை", "நகரம்", "தமிழ்"]
for i, tok in enumerate(tokens):
    nxt = tokens[i+1] if i+1 < len(tokens) else None
    print(tok, "->", ner_tag(tok, nxt))

wiki_ta = wikipediaapi.Wikipedia(user_agent='MyTamilNLPApp/1.0 (https://example.com/myappinfo)', language='ta')

import wikipediaapi
wiki_ta = wikipediaapi.Wikipedia('ta')
wiki_en = wikipediaapi.Wikipedia('en')

def entity_link(entity,fallback_to_en = True):
    entity = entity.strip()
    if not entity:
        return None

    try:
        page = wiki_ta.page(entity)
        if page.exists():
            return {
                "title": page.title,
                "url": page.fullurl,
                "summary": page.summary[:200] + "..." if page.summary else ""
            }

        if fallback_to_en:
            page = wiki_en.page(entity)
            if page.exists():
                return {
                    "title": page.title,
                    "url": page.fullurl,
                    "summary": page.summary[:200] + "..." if page.summary else ""
                }
    except Exception as e:
        return {"error": str(e)}

    return None
print(entity_link)

def deduplicate_sentences(sents):
  return list(dict.fromkeys(sents))

text = extract_text_with_ocr(pdf_path)
text = remove_boilerplate(text)
text = remove_non_tamil(text)
text = remove_numbers_but_keep_years(text)
sentences = sentence_tokenize.sentence_split(text, lang='ta')
sentences = deduplicate_sentences(sentences)
print(text)
print(sentences)

for sent in sentences:
    tokens = indic_tokenize.trivial_tokenize(sent, lang='ta')
    print(f"\nSentence: {sent.strip()}")
    for idx, tok in enumerate(tokens):
        pos = pos_tag(tok)
        next_tok = tokens[idx+1] if idx+1 < len(tokens) else None
        ner = ner_tag(tok, next_tok)
        link = entity_link(tok) if ner in ['PERSON', 'LOCATION'] else None
        print(f"{tok:15} | POS: {pos:8} | NER: {ner:10} | Link: {link}")

!pip install langdetect --quiet

!pip install indic-nlp-library --quiet

from indicnlp.tokenize.sentence_tokenize import sentence_split
from indicnlp.tokenize.indic_tokenize import trivial_tokenize
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
#from indicnlp.morph.analyzer import Analyzer
from indicnlp import common
import unicodedata
import re
from langdetect import detect
import json

!pip install stanza

import stanza
stanza.download('ta')
nlp = stanza.Pipeline('ta', processors='tokenize,pos,lemma', use_gpu=False)

def lemmatize_tamil(text):
    doc = nlp(text)
    lemmas = []
    for sentence in doc.sentences:
        for word in sentence.words:
            lemmas.append(word.lemma)
    return lemmas

text = "அவன் சிறந்த பாடல்கள் எழுதுகிறான்"
print(lemmatize_tamil(text))

INDICNLP_RESOURCES = "/path/to/indicnlp/resources"
common.set_resources_path(INDICNLP_RESOURCES)
#analyzer = Analyzer("ta", INDICNLP_RESOURCES + "/morph/ta/")

# Tamil range regex
tamil_range = re.compile(r'[\u0B80-\u0BFF]+')

# Tamil stopwords (example subset)
stopwords_ta = set(['ஒரு', 'என்று', 'இந்த', 'மற்றும்', 'இது'])

def clean_and_process(text):
    text = unicodedata.normalize('NFKC', text)
    if not tamil_range.search(text):
      return None
    try:
      if detect(text) != 'ta':
        return None
    except:
      return None
    sentences = sentence_split(text, lang='ta')
    cleaned_sentences = []

    for sent in sentences:
        # Word tokenize
        words = trivial_tokenize(sent)
        words = [w for w in words if w not in stopwords_ta]

        lemmatized = []
        for word in words:
            # Without Analyzer, just keep the word
            lemmatized.append(word)

        if lemmatized:
            cleaned_sentences.append(" ".join(lemmatized))

    return cleaned_sentences

input_text = "அவன் ஒரு சிறந்த வீரன். அவன் போரில் வெற்றி பெற்றான்."

output = clean_and_process(input_text)

print(output)

